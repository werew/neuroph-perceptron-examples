\documentclass[twoside,openright,a4paper,11pt,french]{article}
\usepackage[utf8]{inputenc}
\usepackage[french]{babel}
\usepackage[T1]{fontenc}
\usepackage{emptypage}
\usepackage{amsmath}
\usepackage{amssymb}

% Utilisation d'url
\usepackage{url}
\urlstyle{sf}

% Utilisation d'images, stockées dans le répertoire ./pics/
\usepackage{graphicx}
\graphicspath{pics/}

% Définition des marges
\usepackage{geometry}
\geometry{
  left=25mm,
  right=25mm,
  top=25mm,
  bottom=25mm,
  foot=15mm
}
\usepackage{listings}
\usepackage{color}

\definecolor{gray}{rgb}{0.8,0.8,0.8}

\begin{document}

\pagestyle{plain}
\setlength{\parindent}{0pt}
% La page de garde
\include{page-garde}


% La table des matières
\parskip=0pt
\tableofcontents
\clearpage


\vspace{5cm}

%Start content

\section{Fonctions booléennes}

Dans la première section de ce rapport on se propose
d'implementer des fonctions booléennes a l'aide de réseau
de neurones. 

\subsection{"Et" logique}

En mathématiques la conjonction logique $\land$ est un
connecteur qui étant donné deux propositions $a$ et $b$
en forme une nouvelle $a \land b$ qui est vrai seulement
si $a$ et $b$ sont vraies.

\begin{table}[ht]
  \centering
  \begin{tabular}{| c | c | c |}
    \hline
    \textbf{$a$} & \textbf{$b$} & \textbf{$a \land b$}\\
    \hline
    0 & 0  & 0 \\
    \hline
    0 & 1  & 0 \\
    \hline
    1 & 0  & 0 \\
    \hline
    1 & 1  & 1 \\
    \hline
  \end{tabular}
  \caption{Table de vérité de $a \land b$}
  \label{tab:et}
\end{table}




\subsubsection{Un perceptron pour la fonction "et"} 

Est il possible d'utiliser un perceptron pour apprendre la fonction logique
"et"? Bien sur, en effet même un perceptron mono-couche est suffisant pour
obtenir ce resultat.\\

Le perceptron peut être utilisé comme un classificateur linéaire, c'est-à-dire que l'algorithme du
perceptron permet de reconnaître et donc classifier des données (ou points)
linéairement séparables.\\

Un ensemble de points dans le plan caractérisé par deux sous-classes disjointes
de points, est dit linéairement séparable s'il existe une droite qui sépare
complétement les deux sous-classes.\\

En trois dimension, un ensemble est linéairement séparable s'il existe un plan
qui sépare les deux classes. S'il y a quatre ou plus de  dimensions on ne parlera plus de
plan mais d'hyperplan.


\begin{figure}[ht]
\centering
\includegraphics[width=10cm]{./pics/and/and.eps}
\caption{La fonction logique "et" est linéairement séparable}
\label{fig:and}
\end{figure}


Comme le montre la figure \ref{fig:and} la fonction "et" est bien linéairement
séparable. Sachant que tout ensemble linéairement séparable peut être
discriminé par un perceptron mono-couche, on est sûr de pouvoir trouver un perceptron mono-couche qui
engendre la fonction booléenne $\land$.\\

Le perceptron, créé avec Neuroph, qu'on utilisera pour cette fonction 
est très simple et est illustré dans la figure \ref{fig:per_and}.
Le data set pour l'apprentissage contient les mêmes valeurs que la table
\ref{tab:et}.

\begin{figure}[ht]
\centering
\includegraphics[width=4.5cm,height=7cm]{./pics/perc_and.eps}
\caption{Perceptron pour la fonction "et"}
\label{fig:per_and}
\end{figure}

Pendant l'entraînement du perceptron (dont les poids ont été initialisés
aléatoirement) avec un taux d'apprentissage de 0.2 (valeur par
défaut) on obtient une courbe d'apprentissage qui converge toujours à
zéro. La forme de celle-ci dépendra essentiellement des valeurs initiales des
poids. 


\begin{figure}[ht]
\centering
\includegraphics[width=12cm,height=9cm]{./pics/and_error1.eps}
\caption{Exemple de courbe d'apprentissage pour la fonction "et"}
\label{fig:anderr}
\end{figure}

La figure \ref{fig:anderr} montre un exemple de courbe d'apprentissage
obtenu avec les valeurs susmentionnées.

L'apprentissage peut être pensé comme le processus pendant lequel le
réseau de neurones cherche de s'approcher le plus possible du plus petit
taux d'erreur.\\

En effet, la meilleure configuration (valeurs des poids, biais, etc.) pour
notre réseau de neurones correspond au point du minimum global de la fonction
$E$ qui associe à chaque configuration de notre réseau de neurones un coût $C$.\\

$E$ mesure la différence entre le valeur attendue et celle produite et est
souvent calculée comme étant $E(y,y') = \tfrac{1}{2} \lVert y-y'\rVert^2$ ou $y$ est
la valeur attendue et $y'$ est la valeur produite par le réseau.
Mais pourquoi $\lVert y-y'\rVert$ ne suffit simplement pas? 
En effet, le choix d'utiliser une fonction quadratique n'est pas un hasard. 
Un des problèmes les plus importants dans l'apprentissage supervisé d'un réseau
de neurones qui se base sur l'algorithme du gradient (gradient descent) est
celui posé par les points du minimum local de $E$. Le fait que $E$ soit
quadratique permet d'exploiter la convexité des fonctions quadratique pour
minimiser ce problème. Le $\tfrac{1}{2}$ permet de simplifier la fonction lors
de sa dérivation.\\

Comme attendu dans notre cas, le perceptron n'a aucun problème à trouver le
minimum global de $E$, mais dans ce premier exemple on se contente d'entraîner le
réseau de neurones jusqu'à obtenir un taux d'erreur moyen de seulement 1\%. La
figure \ref{fig:andtest1} montre les résultats de l'entraînement:

\begin{figure}[ht]
\centering
\includegraphics[width=9cm,height=3.3cm]{./pics/andtest1.eps}
\caption{Test du perceptron après l'apprentissage de la fonction "et"}
\label{fig:andtest1}
\end{figure}

En utilisant la définition de $E$ on peut vérifier que le coût
moyen est bien proche de $0.01$:

\vspace{1.1cm}

\begin{equation*}
  \begin{aligned}
  \tfrac{1}{4}\times(E(0,0.0061)+E(0,0.1401)+E(0,0.1445)+E(1,0.8185))\\
  = \tfrac{1}{4}\times(\tfrac{0.0061^2}{2}+\tfrac{0.1401^2}{2}+\tfrac{0.1445^2}{2}+\tfrac{0.1815^2}{2}) =
  0.009185965
  \end{aligned}
\end{equation*}

\vspace{1.3cm}

Le résultat obtenu est très proche de celui souhaité, en effet il suffit 
d'introduire un seuil pour obtenir un output qui soit à 0 ou à 1.
Dans Neuroph cela se traduit par l'utilisation de la fonction d'activation step pour le
neurone d'output.


\clearpage
\begin{figure}[ht]
\centering
\includegraphics[width=12cm,height=9cm]{./pics/and_error2.eps}
\caption{Courbe d'apprentissage de "et" en utilisant la fonction step}
\label{fig:anderr1}
\end{figure}



Cette technique nous permet d'obtenir une sortie parfaite qui ne présente
aucun taux d'erreur.



\begin{figure}[ht]
\centering
\includegraphics[width=9cm,height=3.2cm]{./pics/andtest2.eps}
\caption{Test d'erreur après l'apprentissage de la fonction "et"}
\label{fig:andtest2}
\end{figure}


\subsubsection{Différents pas d'apprentissage}

Le pas d'apprentissage détermine la vitesse (pas) avec laquelle notre réseau
cherche à s'approcher du point d'erreur minimum. 
A chaque époque ({\it batch learning}) ou itération ({\it incremental
learning}) les poids du réseau sont mis à jour en utilisant une combinaison du
pas d'apprentissage et du gradient $\nabla E$ ce qui indique la direction du taux
d'accroissement le plus élevé de $E$ et sa pente.\\

Le pas d'apprentissage joue un rôle très important dans la recherche du
minimum global d'une fonction. 
Si un pas d'apprentissage est trop grand, cela risque de ne pas atteindre le minimum global.
Inversement, un pas trop petit augmente le temps nécessaire pour entraîner le réseau et risque de
se bloquer dans un point de minimum locale.\\

Dans notre cas le changement du pas d'apprentissage n'a aucun autre effet que
de changer la vitesse d'apprentissage.


\begin{figure}[ht]
\centering
\includegraphics[width=12cm,height=9cm]{./pics/and_error3.eps}
\caption{Apprentissage fonction "et" avec un pas de 0.1}
\label{fig:anderr3}
\end{figure}

\begin{figure}[ht]
\centering
\includegraphics[width=12cm,height=9cm]{./pics/and_error4.eps}
\caption{Apprentissage fonction "et" avec un pas de 1}
\label{fig:anderr4}
\end{figure}

Avec un pas de 1 (figure \ref{fig:anderr4}) l'erreur totale du perceptron
tombe en-dessous de 0.01 après seulement 17 itérations. En revanche avec 
un pas de 0.1 le nombre d'itérations nécessaires pour obtenir le même
résultat monte à plus de 155 itérations (figure \ref{fig:anderr3}).

\clearpage
\subsection{Equivalence logique}
En mathématiques deux propositions sont dites équivalentes si elles s'impliquent
l'une l'autre: $a \Leftrightarrow b$. La table de vérité de l'équivalence
logique est illustrée par la table \ref{tab:eq}.
Cette table de vérité nous servira de data set pour l'entraînement de tout
les perceptrons de cette partie.


\begin{table}[ht]
  \centering
  \begin{tabular}{| c | c | c |}
    \hline
    \textbf{$a$} & \textbf{$b$} & \textbf{$a \Leftrightarrow b$}\\
    \hline
    0 & 0  & 1 \\
    \hline
    0 & 1  & 0 \\
    \hline
    1 & 0  & 0 \\
    \hline
    1 & 1  & 1 \\
    \hline
  \end{tabular}
  \caption{Table de vérité de $a \Leftrightarrow b$}
  \label{tab:eq}
\end{table}


\subsubsection{Un perceptron mono-couche pour l'équivalence}

Comme pour la fonction booléenne "et" précédente, nous allons essayer de faire
apprendre l'équivalence à un perceptron mono-couche. Pour cela, nous utiliserons le même
perceptron mono-couche que l'on a utilisé pour la fonction "et" (avec deux inputs
et un output). Entrainons ce perceptron sur 1000 itérations:

\begin{figure}[ht]
\centering
\includegraphics[width=12cm,height=9cm]{./pics/eq/mono_eq_def.eps}
\caption{Apprentissage de l'équivalence}
\end{figure}

Nous pouvons observer que le taux d'erreur du perceptron ne converge pas vers
zéro en laissant les paramètres par défaut. Nous pouvons changer le pas
d'apprentissage pour essayer d'obtenir une convergence.
Voyons ce que cela donne avec des pas d'apprentissage de respectivement 0.1 ,
0.01 et 0.001 (figure \ref{fig:eqmono010010001}).

Nous pouvons encore observer que cela ne permet pas de conduire à une convergence.
Le perceptron mono-couche ne semble donc pas adapté pour l'apprentissage de la fonction
d'équivalence. En effet, nous pouvons remarquer que l'équivalence n'est pas une fonction
linéairement séparable (figure \ref{fig:eqnlin}), ceci explique pourquoi un
perceptron mono-couche ne peut pas apprendre de manière correcte la fonction
d'équivalence.

\begin{figure}[ht]
\centering
\includegraphics[width=6cm,height=5cm]{./pics/eq/mono_eq_0.1.eps}
\includegraphics[width=6cm,height=5cm]{./pics/eq/mono_eq_0.01.eps}
\includegraphics[width=6cm,height=5cm]{./pics/eq/mono_eq_0.001.eps}
\caption{Apprentissage équivalence avec un pas de 0.1, 0.01 et 1}
\label{fig:eqmono010010001}
\end{figure}

\begin{figure}[ht]
\centering
\includegraphics[width=10cm]{./pics/eqnonlin/eqlinsep.eps}
\caption{L'équivalence n'est pas linéairement séparable}
\label{fig:eqnlin}
\end{figure}

\clearpage


\subsubsection{Un perceptron multi-couches pour l'équivalence}

Pour permettre l'apprentissage de l'équivalence par un perceptron, il faut donc que
celui-ci soit un perceptron multi-couches.
Nous allons donc créer un perceptron multi-couches qui contiendra une couche cachée de
3 neurones, et toujours avec 2 inputs et un un output (figure \ref{fig:eqmq}).

\begin{figure}[ht]
\centering
\includegraphics[width=5.4cm,height=7cm]{./pics/eq/perceptron_multi.eps}
\caption{Perceptron multi-couches}
\label{fig:eqmq}
\end{figure}

Testons ce perceptron avec les paramètres par défaut:

\begin{figure}[ht]
\centering
\includegraphics[width=12cm,height=9cm]{./pics/eq/multi_eq_def.eps}
\caption{Apprentissage équivalence avec perceptron multi-couches}
\end{figure}

Notre perceptron multi-couches converge en dessous d'un taux d'erreur de 0.1
après environ 3900 itérations.
Nous allons maintenant voir comment le changement de certains paramètres
influence la courbe d'apprentissage. Voyons tout d'abord les effets d'un
changement du pas d'apprentissage en essayant avec des pas d'apprentissage de
respectivement 0.1 , 0.5 et 1 (figure \ref{fig:eqmult01051}).


\begin{figure}[ht]
\centering
\includegraphics[width=6cm,height=5cm]{./pics/eq/multi_eq_0.1.eps}
\includegraphics[width=6cm,height=5cm]{./pics/eq/multi_eq_0.5.eps}
\includegraphics[width=6cm,height=5cm]{./pics/eq/multi_eq_1.eps}
\caption{Apprentissage équivalence avec un pas de 0.1, 0.5 et 1}
\label{fig:eqmult01051}
\end{figure}

\begin{figure}[ht]
\centering
\includegraphics[width=6cm,height=5cm]{./pics/eq/multi-5_eq_def.eps}
\includegraphics[width=6cm,height=5cm]{./pics/eq/multi-10_eq_def.eps}
\includegraphics[width=6cm,height=5cm]{./pics/eq/multi-20_eq_def.eps}
\caption{Apprentissage équivalence avec 5, 10 et 20 neurones cachés}
\label{fig:eqmult51020}
\end{figure}
\clearpage

Ces 3 graphiques permettent de mettre en évidence le fait que plus le pas
d'apprentissage est grand, plus les variations ont de chance d'être soudaine;
et plus le pas est petit, plus la courbe sera uniforme (avec des variations
moins prononcées). Cela correspond bien à nos attentes étant donné que le 
pas d'apprentissage est utilisé pour déterminer la vitesse de déplacement
sur la courbe d'erreur.  

Changeons à présent le nombre de neurone dans la couche caché.  Nous allons
partir d'un réseau de 5 neurones, et les doubler à chaque fois. Nous
pourrons ainsi voir assez rapidement si le nombre de neurone influe grandement
sur les résultats. La figure \ref{fig:eqmult51020} représente donc
respectivement les résultats pour des perceptrons de 5, 10 et 20 neurones dans
une couche cachée.
Augmenter le nombre de neurones dans la couche cachée semble diminuer le nombre
d'itération nécessaire pour que la courbe converge.
La courbe semble aussi être plus "uniforme".

Essayons maintenant d'ajouter plus de couche cachée. Voyons la
courbe que l'on obtient avec un perceptron de deux couches cachées avec 3 
neurones chacunes.

\begin{figure}[ht]
\centering
\includegraphics[width=12cm,height=9cm]{./pics/eq/multi_3_3_def.eps}
\caption{Apprentissage équivalence avec 2 couches cachées de 3 neurones chacunes}
\end{figure}


Rajouter une couche cachée ne semble pas améliorer le nombre d'itération
nécessaire par rapport à un perceptron à une seule couche cachée.  Essayons de
rajouter plus de neurones dans la couche cachée (figures \ref{fig:eqmult2cc4}
et \ref{fig:eqmult2cc6}). Rajouter des neurones dans les couches intermédiaires
n'améliore pas le résultat non plus.

Enfin comme vu précédement, le changement de la fonction d'activation par la
fonction step nous permet d'introduire un seuil et donc d'obtenir un résultat
binaire (figure \ref{fig:eqstep}) qui correpsond bien à la nature de notre
fonction. 


\begin{figure}[ht]
\centering
\includegraphics[width=12cm,height=9cm]{./pics/eq/multi_4_4_def.eps}
\caption{Apprentissage équivalence avec 2 couches cachées de 4 neurones chacunes}
\label{fig:eqmult2cc4}
\end{figure}

\begin{figure}[ht]
\centering
\includegraphics[width=12cm,height=9cm]{./pics/eq/multi_6_6_def.eps}
\caption{Apprentissage équivalence avec 2 couches cachées de 6 neurones chacunes}
\label{fig:eqmult2cc6}
\end{figure}

\begin{figure}[ht]
\centering
\includegraphics[width=9cm,height=3cm]{./pics/eq/stepeq.eps}
\caption{Résultat du test avec la fonction d'activation step}
\label{fig:eqstep}
\end{figure}

\clearpage



\section{Apprentissage de la fonction: $f(x) = x^2$}

Il est temps de tester si un perceptron (mono-couche ou multi-couches) est
capable d'apprendre une fonction un peu plus complexe que celle que l'on a vu
jusqu'à présent, la fonction carré: $f(x) = x^2$ \\

Plusieurs différences sont à noter par rapport aux fonctions précédentes.
Si précédement, le domaine de nos fonctions binaires était réduit à ${0,1}$, cette
fois nous sommes confronté à une fonction continue définie dans tout $\mathbb{R}$.
Une autre différence est liée au type et au nombre d'inputs, ce qui change
considérablement le paradigme utilisé par notre réseau.

Un concept que nous n'avons pas introduit jusqu'à présent est l'existence de plusieurs
types de données pour l'apprentissage, et d'autant de paradigmes d'apprentissage qui en
découlent. Les deux principaux types de données avec lesquels on doit faire
face sont:

\begin{itemize}
\item des variables catégorielles (dites aussi qualitatives) - où il y a généralement
      un ou plusieurs cas qui peuvent être regroupés en différentes catégories 
     (par exemple: haut, bas ou rouge, vert, bleu, etc.)
\item des données quantitatives qui se présentes comme une mesure numérique d'un attribut
\end{itemize}

Le type d'apprentissage que l'on utilise pour reconnaître et classifier des
données appartenant à la première famille sont appelées "classification". 
En revanche l'apprentissage supervisé avec des données quantitatives est appelé
"régression".\cite{kindsNN}\\

Pour récapituler: la régression consiste à estimer ou à prédire une réponse,
la classification est l'identification de l'appartenance à un groupe.\\

Dans nos exemples précédents ("et" logique et équivalence) on a simplement
traité des classifications binaires. Cette fois on est confronté avec des données
de type quantitative.


\subsection{Un perceptron pour la fonction $f(x) = x^2$}
Un perceptron n'est pas le moyen le plus efficace pour calculer cette 
fonction, mais peut il le faire?\\

Comme nous l'avons vu précédement, un perceptron mono-couche peut être utilisé sans aucun
problème pour distinguer des données linéairement séparables, mais il échoue 
à reconnaître des schémas un peu plus complexe. Il n'est donc 
probablement pas adapté à notre problème.\\

Le théorème de l'approximation universelle (aussi connu sous le nom du théorème de Cybenko) 
nous dit qu'un perceptron multi-couches avec une seule couche cachée (et un nombre
fini de neurones) peut approximer n'importe quelle fonction continue dans un 
intervalle de $\mathbb{R}$.
\footnote{
La démonstration mathématiques de ce théorème est assez longue et complexe. Une approche visuelle
qui se prête à une demonstration beaucoup plus intuive est proposé à \cite{visuniprof}.
}
\cite{cybthm}

Pour l'apprentissage de la fonction carré nous allons donc utiliser un perceptron
multi-couches avec une seule couche cachée et on cherchera à lui faire apprendre
la fonction carré dans un intervalle de $\mathbb{R}$.

Pour commencer on pourra utiliser un perceptron avec 3 neurones cachés comme
celui que l'on a utilisé précédement pour l'équivalence. Notre but sera
d'approximer la fonction carré dans l'intervalle $[1,100]$.

Dans notre exemple chaque neurone de notre perceptron utilisera la fonction
d'activation sigmoïde, c'est pour cette raison que l'on normalisera les inputs
d'apprentissage pour les borner entre l'intervalle $[0,1]$ (codomaine de la
fonction sigmoïde).

\subsubsection{3 neurones cachés}

Pour ne pas surcharger notre  perceptron on pourrait commencer à tester
ses capacités d'apprentissage avec un dataset de dimensions réduites (table
\ref{tab:fqt1}). On rappelle que pour avoir un data set compris entre 0 et 1
nous avons normalisé notre data set.

\begin{table}[ht]
  \centering
  \begin{tabular}{| c | c |}
    \hline
    \textbf{$x$} & \textbf{$f(x)$}\\
    \hline
    0 & 0 \\
    \hline
    10 & 100 \\
    \hline
    20 & 400 \\
    \hline
    ... & ... \\
    \hline
    100 & 10000 \\
    \hline
  \end{tabular}
  \caption{Fontion carré: dataset 1}
  \label{tab:fqt1}
\end{table}

Avec les valeurs par défaut pour l'apprentissage, notre réseau arrive à obtenir
un bon taux d'erreur dans un temps raisonable comme le montre la figure
\ref{fig:sqtest1}. Après ce premier test notre réseau nous offre déjà une
approximation appréciable de la fonction carré (figure \ref{fig:chartsqtest1}).

\begin{figure}[ht]
\centering
\includegraphics[width=12cm,height=9cm]{./pics/sqtest1.eps}
\caption{Apprentissage fonction carré: 3 neurones, dataset 1}
\label{fig:sqtest1}
\end{figure}

\begin{figure}[ht]
\centering
\includegraphics[width=12cm,height=9cm]{./pics/chartsqtest1.eps}
\caption{Résultat apprentissage: 3 neurones, dataset 1}
\label{fig:chartsqtest1}
\end{figure}

A présent nous pouvons essayer d'augmenter la précision de notre dataset
pour essayer d'obtenir une meilleure approximation. Pour notre 
prochain test on essayera de doubler la précision de notre dataset.
Les figures \ref{fig:sqtest2} et \ref{fig:chartsqtest2} nous montre 
une amélioration sensible des performances de notre réseau. 



\begin{figure}[ht]
\centering
\includegraphics[width=12cm,height=9cm]{./pics/sqtest2.eps}
\caption{Apprentissage fonction carré: 3 neurones, dataset 2}
\label{fig:sqtest2}
\end{figure}

\begin{figure}[ht]
\centering
\includegraphics[width=12cm,height=9cm]{./pics/chartsqtest2.eps}
\caption{Résultat apprentissage: 3 neurones, dataset 2}
\label{fig:chartsqtest2}
\end{figure}

Par contre on se rend très vite compte que notre réseau commence à être aux limites
de ces capacités.
Après un certain temps, la vitesse d'apprentissage ralentit beaucoup, 
ce qui implique plusieurs milliés d'itérations pour améliorer de façon 
significative notre approximations (la figure \ref{fig:chartsqtest3} nous
montre les résultats en poursuivant l'entraînement de notre réseau jusqu'à
environ 30.000 époques).La fonction d'erreur est déjà assez proche de son minimum 
global, c'est donc le moment de passer à un réseau un peu plus grand
et donc avec plus de capacites de stockage.


\begin{figure}[ht]
\centering
\includegraphics[width=12cm,height=9cm]{./pics/chartsqtest3.eps}
\caption{Résultat apprentissage: 3 neurones, dataset 2, 30.000 iterations}
\label{fig:chartsqtest3}
\end{figure}


\subsubsection{4 neurones cachés}

Avec un réseau de 4 neurones cachés, plus ou moin 10.000 itérations
sont suffisantes pour obtenir un resultat qui dépasse celui obtenu
précedement avec une couche de 3 neuronés et 30.000 itérations 
(figures \ref{fig:sqtest4} et \ref{fig:chartsqtest4}).


\begin{figure}[ht]
\centering
\includegraphics[width=12cm,height=9cm]{./pics/sqtest4.eps}
\caption{Apprentissage fonction carré: 4 neurones, dataset 2, 10.000 itérations}
\label{fig:sqtest4}
\end{figure}

\begin{figure}[ht]
\centering
\includegraphics[width=12cm,height=9cm]{./pics/chartsqtest4.eps}
\caption{Résultat apprentissage: 4 neurones, dataset 2, 10.000 itérations}
\label{fig:chartsqtest4}
\end{figure}

Avec un dataset encore plus grand (100 inputs) et après environ
30.000 itérations, on arrive à obtenir une approximation suffisament
précise (figure \ref{fig:chartsqtest5}) comme supposé par le théorème de Cybenko:
un perceptron multi-couches avec une seule couche cachée est suffisant pour
approximer la fonction carré (comme n'importe quelle fonction continue) 
et la précision dépend strictement du nombre de neurones dans la couche cachée.

\begin{figure}[ht]
\centering
\includegraphics[width=12cm,height=9cm]{./pics/chartsqtest5.eps}
\caption{Résultats apprentissage: 4 neurones, dataset 3, 30.000 itérations}
\label{fig:chartsqtest5}
\end{figure}


\clearpage
\section{Conclusion}
Comme nous avons pu le constater dans la première partie de ce rapport, les
perceptrons pouvent être utilisés efficacement dans la classification des données.
En particulier un perceptron mono-couche est suffisant pour déterminer une
catégorisation entre des ensembles de données linéairement séparables.\\

Nous avons aussi pu voir que les perceptrons pouvent aussi être utilisés dans
le calcul et l'approximation de n'importe quelle fonction continue, ce résultat
est résumé par le théorème de Cybenko.  Une couche avec plusieurs neurones
comporte une capacité de stockage supérieure et permet donc une meilleure
approximation.\\

Pendant l'apprentissage on essaye de s'approcher le plus possible du minumim
global de la fonction d'erreur.\\

Le pas d'apprentissage détermine la taille de chaque avancement pendant la
recherche du minumim dans la fonction d'erreur.  Un grand pas d'apprentissage
permet d'entraîner le perceptron plus rapidement mais risque de manquer le
minimum global. En revanche un petit pas d'apprentissage permet de faire une
recherche plus fine et de s'approcher plus précisement du minimum global. Il
risque cependant d'être bloqué dans un minimum local.\\




%End content
\addcontentsline{toc}{section}{Références}
\bibliographystyle{plain}
\bibliography{rapport}

\end{document}
