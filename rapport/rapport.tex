\documentclass[twoside,openright,a4paper,11pt,french]{article}
\usepackage[utf8]{inputenc}
\usepackage[french]{babel}
\usepackage[T1]{fontenc}
\usepackage{emptypage}
\usepackage{amsmath}
\usepackage{amssymb}

% Utilisation d'url
\usepackage{url}
\urlstyle{sf}

% Utilisation d'images, stockées dans le répertoire ./pics/
\usepackage{graphicx}
\graphicspath{pics/}

% Définition des marges
\usepackage{geometry}
\geometry{
  left=25mm,
  right=25mm,
  top=25mm,
  bottom=25mm,
  foot=15mm
}
\usepackage{listings}
\usepackage{color}

\definecolor{gray}{rgb}{0.8,0.8,0.8}

\begin{document}

\pagestyle{plain}
\setlength{\parindent}{0pt}
% La page de garde
\include{page-garde}


% La table des matières
\parskip=0pt
\tableofcontents
\clearpage


\vspace{5cm}

%Start content

\section{Fonctions booléennes}

Dans cette premiere section de ce rapport on se propose
de implementer des fonctions booléennes a l'aide des reseaux
de neurones. 

\subsection{"Et" logique}

En matematique la conjonction logique $\land$ est un
connecteur logique que, etant donne deux propositions $a$ et $b$
forme une nouvelle propositions $a \land b$ qui est vrai seulement
si $a$ et $b$ sont vraies.

\begin{table}[ht]
  \centering
  \begin{tabular}{| c | c | c |}
    \hline
    \textbf{$a$} & \textbf{$b$} & \textbf{$a \land b$}\\
    \hline
    0 & 0  & 0 \\
    \hline
    0 & 1  & 0 \\
    \hline
    1 & 0  & 0 \\
    \hline
    1 & 1  & 1 \\
    \hline
  \end{tabular}
  \caption{Table de verite de $a \land b$}
  \label{tab:et}
\end{table}




\subsubsection{Un perceptron pour $\land$} 

Est it possible d'utiliser un perceptron pour apprendre la fonction logique
"et"? Bien sur, en effet meme un perceptron mono couche est sufficent pour
obtenir cet resultat.\\

Le perceptron peut etre utilise comme un classifieur linéaire, cad. l'algorithme du
perceptron perment de reconnaitre et donc classifier des donnes (ou points)
linearement separables.\\

Un ensemble de points dans le plan caracterise par deux sous-classes disjointes
de points, est dit linearement separable si il existe une droite qui separe
completement les deux sous-classes.\\

Dans trois dimension un ensemble est linearement separable si il existe un plan
qui separe les deux classes. Dans quatre ou plus dimensions on ne parlera plus de
plan mais de hyperplan.


\begin{figure}[ht]
\centering
\includegraphics[width=10cm]{./pics/and/and.eps}
\caption{La fonction logique "et" est linearement separable}
\label{fig:and}
\end{figure}


Comme montre la figure \ref{fig:and} la fonction "et" est bien linearement
separable. Etant donne que tout ensemble linéairement séparable peut être
discriminé par un perceptron on est sur de pouvoir trouver un perceptron qui
engendre la fonction booléenne $\land$.\\

Le perceptron cree avec Neuroph que on utilisera pour cette fonction 
est vraiment tres simple et est inllustre dans la figure \ref{fig:per_and}.
Le data set pour l'apprentissage contiens les meme valeurs de la table
\ref{tab:et}.

\begin{figure}[ht]
\centering
\includegraphics[width=4.5cm,height=7cm]{./pics/perc_and.eps}
\caption{Perceptron pour la fonction "et"}
\label{fig:per_and}
\end{figure}

Pendant l'entrainement du perceptron (dont les poids ont ete initialize avant
aleatoireament) avec des avec un taux d'apprentissage de 0.2 (valeur par
default) on obtient une courbe d'apprentissage qui converge toujours a
zero. La forme de celle-ci dependra essentialement des valeurs initiales des
poids. 


\begin{figure}[ht]
\centering
\includegraphics[width=12cm,height=9cm]{./pics/and_error1.eps}
\caption{Exemple de courbe d'apprentissage pour la fonction "et"}
\label{fig:anderr}
\end{figure}

La figure \ref{fig:anderr} montre un exemple de courbe d'apprentissage
obtenu avec les valeurs susmentionne.\\

L'apprentissage peut etre pense comme le processus pendat le quelle la
reseau de neurons cherche de s'approcher le plus possible au plus petit
taux d'erreur.\\

En effet, la meilleure configuration (valeurs des poids, biais, etc.) pour
notre reseau de neurons corresponde au point de minimum globale de la fonction
$E$ qui associe a toute configuration de notre reseau de neurons un cout $C$.\\

$E$ mesure la difference entre le valeur attendu et celui produit et est
souvent calcule comme etant $E(y,y') = \tfrac{1}{2} \lVert y-y'\rVert^2$ ou $y$ est
le valeur attendu et $y'$ est le valeur produit par la reseau.
Mais pourquoi pas plus simplement $\lVert y-y'\rVert$? 
En effet, la choix d'utiliser une fonction quadratique n'est pas casuelle. 
Un des problemes plus importantes dans l'apprentissage supervisee d'une reseau
de neurones qui se base sur l'algorithme du gradient (gradient descent) est
celui pose par les points de minimum locales de $E$. Le fait que $E$ soit
quadratique permet d'exploiter la convexite des fonctions quadratique pour
minimizer ce probleme. Le $\tfrac{1}{2}$ permet de simplifier la fonction lors
de sa derivation.\\

Comme attendu, dans notre cas le perceptron n'a aucun probleme a trouver le
minimum globale de $E$ mais dans ce premiere exemple on se contente d'entrainer la
reseau de neurons jusq'a obtenir un taux d'erreur moyen de seulement 1\%. La
figure \ref{fig:andtest1} montre les resultat de l'entrainement:

\begin{figure}[ht]
\centering
\includegraphics[width=7.3cm,height=2.3cm]{./pics/andtest1.eps}
\caption{Test d'erreur apres l'apprentissage de la fonction "et"}
\label{fig:andtest1}
\end{figure}

En utilisant la definition de $E$ on peut verifier que le cout
moyen est bien tres proche de $0.01$:

\begin{equation*}
  \begin{aligned}
  \tfrac{1}{4}\times(E(0,0.0061)+E(0,0.1401)+E(0,0.1445)+E(1,0.8185))\\
  = \tfrac{1}{4}\times(\tfrac{0.0061^2}{2}+\tfrac{0.1401^2}{2}+\tfrac{0.1445^2}{2}+\tfrac{0.1815^2}{2}) =
  0.009185965
  \end{aligned}
\end{equation*}

Le resultat obtenu est tres proche de celui souhaite, en effet il suffit de 
introduire une seuille pour obtenir un output qui soit un 0 ou un 1.
Dans Neuroph ca se traduit par utiliset la fonction d'activation step pour le
neuron d'output.


\begin{figure}[ht]
\centering
\includegraphics[width=12cm,height=9cm]{./pics/and_error2.eps}
\caption{Courbe d'apprentissage de "et" en utilisant la fonction step}
\label{fig:anderr}
\end{figure}



Cette technique nous perment d'obtenir une sortie parfaite qui ne presente
aucun taux d'erreur.



\begin{figure}[ht]
\centering
\includegraphics[width=7.3cm,height=2.3cm]{./pics/andtest2.eps}
\caption{Test d'erreur apres l'apprentissage de la fonction "et"}
\label{fig:andtest2}
\end{figure}


\subsubsection{Differents pas d'apprentissage}

Le pas d'apprentissage determine la vitesse (pas) avec la quelle notre reseau
cherche a s'approcher au point d'erreur minimum. 
A chaque epoque ({\it batch learning}) ou a chaque iteration ({\it incremental
learning}) les poids de la reseau sont mise a jour en utilisant une combinaison du
pas d'apprentissage et le gradient $\nabla E$ qui indique la direction du taux
d'accroissement le plus élevé de $E$ et sa pente.\\

Le pas d'apprentissage joue un role tres importante dans la recherche du
minimum globale d'une fonction. 
Si un pas d'apprentissage trop grand risque de ne pas atteindre le minimum globale
un pas trop petit augment le temps necessaire a entrainer le reseau et risque de
se bloquer dans un point de minimum locale.\\

Dans notre cas le changement du pas d'apprentissage n'a aucun effect que celui 
de changer la vitesse d'apprentissage.


\begin{figure}[ht]
\centering
\includegraphics[width=12cm,height=9cm]{./pics/and_error3.eps}
\caption{Apprentissage fonction "et" avec un pas de 0.1}
\label{fig:anderr3}
\end{figure}

\begin{figure}[ht]
\centering
\includegraphics[width=12cm,height=9cm]{./pics/and_error4.eps}
\caption{Apprentissage fonction "et" avec un pas de 1}
\label{fig:anderr4}
\end{figure}

Avec un pas de 1 (figure \ref{fig:anderr4}) l'erreur totale du perceptron
tombe au-dessus de 0.01 apres seulement 17 iterations, par contre avec 
un pas de 0.1 le nombre d'iterations necessaires pour obtenir le meme
resultat monte a plus de 155 iterations (figure \ref{fig:anderr3}).

\subsection{Equivalence logique}
En matematique deux propositions sont dites equivalentes si l'un implique
l'autre: $a \Leftrightarrow b$. La table de verite de l'équivalence logique
est illustre par la table \ref{tab:eq}.
Cette table de vérité nous servira de data set pour l'entrainement de tout
les perceptron de cette partie.


\begin{table}[ht]
  \centering
  \begin{tabular}{| c | c | c |}
    \hline
    \textbf{$a$} & \textbf{$b$} & \textbf{$a \Leftrightarrow b$}\\
    \hline
    0 & 0  & 1 \\
    \hline
    0 & 1  & 0 \\
    \hline
    1 & 0  & 0 \\
    \hline
    1 & 1  & 1 \\
    \hline
  \end{tabular}
  \caption{Table de verite de $a \Leftrightarrow b$}
  \label{tab:eq}
\end{table}

\subsubsection{Un perceptron monocouche pour $\Leftrightarrow$}

Comme pour la fonction booléenne "et" précédente nous allons essayer de faire
apprendre l'équivalence à un perceptron monocouche. Pour ca, nous utiliserons le meme
perceptron monocouche que on a utilisee pour la fonction "et" (avec deux inputs
et un output).Entrainons ce perceptron avec le data set sur 1000 itérations:

\begin{figure}
\centering
\includegraphics[width=12cm,height=9cm]{./pics/eq/mono_eq_def.eps}
\caption{Apprentissage de l'équivalence}
\end{figure}

Nous pouvons observer qu'il ne semble pas avoir de convergence avec ce
perceptron en laissant les paramètre par défaut. Nous pouvons changer le pas
d'apprentissage pour essayer d'avoir une convergence.

Voyons se que cela donne avec des pas d'apprentissage de respectivement 0.1 ,
0.01 et 0.001:

\begin{figure}[ht]
\centering
\includegraphics[width=8cm,height=5cm]{./pics/eq/mono_eq_0.1.eps}
\includegraphics[width=8cm,height=5cm]{./pics/eq/mono_eq_0.01.eps}
\includegraphics[width=8cm,height=5cm]{./pics/eq/mono_eq_0.001.eps}
\caption{Apprentissage équivalence avec un pas de 0.1, 0.01 et 1}
\end{figure}

Nous pouvons encore observer que cela ne permet pas de conduire à une convergence.

Le perceptron monocouche ne semble donc pas adapté pour l'apprentissage de la fonction
d'équivalence. En effet, nous pouvons remarquer que l'équivalence n'est pas une fonction
linéairement séparable (figure \ref{fig:eqnlin}).

\begin{figure}[ht]
\centering
\includegraphics[width=6cm,height=3cm]{./pics/eqnonlin/eqlinsep.eps}
\caption{L'équivalence n'est pas linéairement separable}
\label{fig:eqnlin}
\end{figure}

Ceci explique pourquoi un perceptron monocouche ne peut pas apprendre de manière correcte
la fonction d'équivalence.

\subsubsection{Un perceptron multicouche pour $ \Leftrightarrow $}

Pour permettre l'apprentissage de l'équivalence par un perceptron, il faut donc que
celui-ci soit un perceptron multicouche.
Nous allons donc créer un perceptron multicouche qui contiendra une couche cachée de
3 neurones, et avec toujours 2 inputs et un un output (figure \ref{fig:eqmq}).

\begin{figure}[ht]
\centering
\includegraphics[width=5cm,height=7cm]{./pics/eq/perceptron_multi.eps}
\caption{Perceptron multi-couche}
\label{fig:eqmq}
\end{figure}

Testons ce perceptron avec le dataset et les paramètres par défaut:

\begin{figure}[ht]
\centering
\includegraphics[width=12cm,height=9cm]{./pics/eq/multi_eq_def.eps}
\caption{Apprentissage équivalence avec perceptron multi-couche}
\end{figure}

Notre perceptron multi-couche converge en dessous d'un taux d'erreur de 0.1
après environ 3900 itérations.
Nous allons maintenant voir comment le changement de certains paramètres
influence la courbe d'apprentissage. Voyons tout d'abord les effets d'un
changement du pas d'apprentissage en essayant avec des pas d'apprentissages de
respectivement 0.1 , 0.5 et 1.

\begin{figure}[ht]
\centering
\includegraphics[width=6cm,height=3cm]{./pics/eq/multi_eq_0.1.eps}
\caption{Apprentissage équivalence avec un pas de 0.1}
\end{figure}

\begin{figure}[ht]
\centering
\includegraphics[width=6cm,height=3cm]{./pics/eq/multi_eq_0.5.eps}
\caption{Apprentissage équivalence avec un pas de 0.5}
\end{figure}

\begin{figure}[ht]
\centering
\includegraphics[width=6cm,height=3cm]{./pics/eq/multi_eq_1.eps}
\caption{Apprentissage équivalence avec un pas de 1}
\end{figure}

Changeons à présent le nombre de neurone dans la couche caché.
Nous allons partir d'un réseau de 5 neurones, et le doublé d'un perceptron
à l'autre. Nous pourrons ainsi voir assez rapidement si le nombre de neurone
influe grandement sur les résultats.
Les graphiques ci dessus représentant donc respectivement les résultats pour
des perceptrons de 5, 10 et 20 neurone dans une couche caché.

\begin{figure}[ht]
\centering
\includegraphics[width=6cm,height=3cm]{./pics/eq/multi-5_eq_def.eps}
\caption{Apprentissage fonction "et" avec un pas de 1}
\end{figure}

\begin{figure}[ht]
\centering
\includegraphics[width=6cm,height=3cm]{./pics/eq/multi-10_eq_def.eps}
\caption{Apprentissage fonction "et" avec un pas de 1}
\end{figure}

\begin{figure}[ht]
\centering
\includegraphics[width=6cm,height=3cm]{./pics/eq/multi-20_eq_def.eps}
\caption{Apprentissage fonction "et" avec un pas de 1}
\end{figure}


Augmenter le nombre neurone dans la couche caché semble diminuer le nombre
d'itération nécessaire pour que la courbe converge.
La courbe semble aussi être plus "lisse".

Essayons maintenant maintenant d'ajouter plus de couche cachées. Voyons la
courbe que l'on obtient avec un perceptron de deux couches cachées avec 3 
neurones chacune.

\begin{figure}[ht]
\centering
\includegraphics[width=12cm,height=9cm]{./pics/eq/multi_3_3_def.eps}
\caption{Apprentissage fonction "et" avec un pas de 1}
\end{figure}


Rajouter une couche cachée supplémentaire ne semble pas améliorer le nombre d'itération
nécessaire par rapport à un perceptron à une seule couche cachée.

\begin{figure}[ht]
\centering
\includegraphics[width=6cm,height=3cm]{./pics/eq/multi_3_3_def.eps}
\caption{Apprentissage fonction "et" avec un pas de 1}
\end{figure}

\begin{figure}[ht]
\centering
\includegraphics[width=6cm,height=3cm]{./pics/eq/multi_4_4_def.eps}
\caption{Apprentissage fonction "et" avec un pas de 1}
\end{figure}

\begin{figure}[ht]
\centering
\includegraphics[width=6cm,height=3cm]{./pics/eq/multi_6_6_def.eps}
\caption{Apprentissage fonction "et" avec un pas de 1}
\end{figure}

\begin{figure}[ht]
\centering
\includegraphics[width=6cm,height=3cm]{./pics/eq/multi_3_4_def.eps}
\caption{Apprentissage fonction "et" avec un pas de 1}
\end{figure}
\clearpage

Rajouter des neurones dans les couche intermédiaire n'améliore pas le résultat
non plus.

\begin{figure}[ht]
\centering
\includegraphics[width=6cm,height=3cm]{./pics/eq/multi_3_3_3_def.eps}
\caption{Apprentissage fonction "et" avec un pas de 1}
\end{figure}


Et rajouter encore une couche fait croitre la courbe jusqu'a ce qu'elle se
stabilise et ne semble plus diminuer.


\section{Apprentissage de la fonction: $f(x) = x^2$}

Il est le moment de tester si un perceptron (mono-couche ou multi-couches) est
capable de apprendre une fonction un peu plus complexe que celle qu'on a vu avant, 
la fonction carre: $f(x) = x^2$ !\\

Plusieurs difference par rapport aux fonction precedentes sautent aux yeux.
Si avant le domaine de notre fonctions binaire etait reduit a ${0,1}$, cette
fois nous sommes confrontes a une fonction continue definie dans tout $\mathbb{R}$.
%TODO if there is time: can we implement a NN in R? %
Un autre difference est cela lie aux type et nombre d'inputs, et change
considerablement le paradigme utilise par notre reseau:\\

Une concept qu'on a pas introduit jusq'a maintenant est l'existence de plusieur
types de donnes pour l'apprentissage et autant de paradigmes s'apprentissage qui en
decoulent. Les deux principales types de donnes avec le quelle on doit faire
face sont:

\begin{itemize}
\item Des variables catégorielles (dit aussi qualitative) - ou il y a generalement
      un ou plusieurs cases qui peuvent etre groupe en differents categories 
     (par example: haut, bas ou rouge, vert, bleu, etc.)
\item Des donne quantitatives qui se presente comme mesuration numeriques d'un attribut
\end{itemize}

On appelle les types d'apprentissage utilise pour reconnaitre et classifier des donnes
appartenant a la premiere famille "classification". De l'autre cote l'apprentissage
supervise avec des donnes quantitatives est appelle "regression".\cite{kindsNN}\\

Pour recapituler: la régression consiste à estimer ou à prédire une réponse,
la classification est l'identification de l'appartenance a un groupe.\\

Dans notre example precedents ("et" logique et équivalence) on a simplement
traite des classification binaires, cette fois on est confronte avec des donnes
de type quantitative.


\subsection{Un perceptron pour la fonction $f(x) = x^2$}
Un perceptron n'est pas peut etre le moyen plus efficace pour calculer cette 
fonction, mais, peut il le faire?\\

Comme on l'a vu avant un perceptron mono-couche peut etre utilise sans aucun
probleme pour distinguer des donnes linearement separables, mais il echoue 
a reconnaitre des schema un petit peu plus complexes. Il n'est donc pas 
probabilment adaptee a notre probleme.\\

Le theoreme de l'approximation universelle (aussi connu comme theoreme de Cybenko) 
nous dis que un perceptron multi-couches avec une seule couche cache (avec un nombre
fini de neurons) peut approximer n'importe quelle fonction continue dans un 
intervalle de $\mathbb{R}$.
\footnote{
La demostration mathematique de ce theoreme est assez longue et complexe. Un approche visuelle
qui se preste a une demostration beaucoup plus intuive est propose at \cite{visuniprof}.
}
\cite{cybthm}

Pour l'apprentissage de la fonction carré nous donc utiliser un perceptron
multi-couche avec une seule couche cache et on cherchera de lui faire apprendre
la fonction carre dans un intervalle de $\mathbb{R}$.

Pour commencer on pourra utiliser un perceptron avec 3 neurons cache comme
celui que on a utilise precedement pour l'équivalence. Notre mission sera de
approximer la fonction carre dans l'intervalle $[1,100]$.

Dans notre example chaque neuron de notre perceptron utilisera la fonction
d'activation sigmoide pour cette raison on normalisera les inputs
d'apprentissage pour les borner entre l'intervalle $[0,1]$ (codomaine de la
fonction sigmoide).

\subsubsection{3 neurons cache}

Pour ne pas surcharger notre petite perceptron on pourrait commencer a tester
ses capacites d'apprentissage avec un dataset de dimensions reduites (table
\ref{tab:fqt1}). On rappelle que pour avoir un data set compris entre 0 et 1
nous avons normalisé notre data set.

\begin{table}[ht]
  \centering
  \begin{tabular}{| c | c |}
    \hline
    \textbf{$x$} & \textbf{$f(x)$}\\
    \hline
    0 & 0 \\
    \hline
    10 & 100 \\
    \hline
    20 & 400 \\
    \hline
    ... & ... \\
    \hline
    100 & 10000 \\
    \hline
  \end{tabular}
  \caption{Fontion carre: dataset 1}
  \label{tab:fqt1}
\end{table}

Avec les valeurs par defaut pour l'apprentissage notre reseau arrive a obtenir
un bon taux d'erreur dans un temps raisonable comme montre par la figure
\ref{fig:sqtest1}. Apres ce premier test notre reseau nous offre deja une
appreciable approximation de la fonction carree (figure \ref{fig:chartsqtest1}).

\begin{figure}[ht]
\centering
\includegraphics[width=12cm,height=9cm]{./pics/sqtest1.eps}
\caption{Apprentissage fonction carree: 3 neurons, dataset 1}
\label{fig:sqtest1}
\end{figure}

\begin{figure}[ht]
\centering
\includegraphics[width=12cm,height=9cm]{./pics/chartsqtest1.eps}
\caption{Resultats apprentissage: 3 neurons, dataset 1}
\label{fig:chartsqtest1}
\end{figure}

Maintenant on peut essayer de augmenter la precisions de notre dataset
pour chercher d'obtenir une meilleure approximation. Pour notre 
prochaine test on essayra de doubler la precision de notre dataset.
Les figure \ref{fig:sqtest2} et \ref{fig:chartsqtest2} nous montront une sensible 
emeilloration des prestations de notre reseau. 



\begin{figure}[ht]
\centering
\includegraphics[width=12cm,height=9cm]{./pics/sqtest2.eps}
\caption{Apprentissage fonction carree: 3 neurons, dataset 2}
\label{fig:sqtest2}
\end{figure}

\begin{figure}[ht]
\centering
\includegraphics[width=12cm,height=9cm]{./pics/chartsqtest2.eps}
\caption{Resultats apprentissage: 3 neurons, dataset 2}
\label{fig:chartsqtest2}
\end{figure}

Par contre on se rend tres vite compte que notre reseau commence a fatiguer.
Apres un certains temps la vitesse d'apprentissage rallentisse beaucoup, 
ce que comporte plusieurs millie de iterations pour emeilliorer de facon 
significative notre approximations (la figure \ref{fig:chartsqtest3} nous
montre les resultats en poursuivant l'entrainement de notre reseau jusq'a
environ 30.000 epoques).La fonction d'erreur est deja assez proche de son minimum 
globale il est arrive le moment de passer a une reseau un peu plus grande 
avec donc plus de capacites de stockage.


\begin{figure}[ht]
\centering
\includegraphics[width=12cm,height=9cm]{./pics/chartsqtest3.eps}
\caption{Resultats apprentissage: 3 neurons, dataset 2, 30.000 iteractions}
\label{fig:chartsqtest3}
\end{figure}


\subsubsection{4 neurons caches}

Avec une reseau a 4 neurons caches plus ou moin 10.000 iterations
sont deja suffisantes pour obtenir un resultat qui depasse celui obtenu
avant avec une couche de 3 neurons et 30.000 iterations 
(figures \ref{fig:sqtest4} et \ref{fig:chartsqtest4}).


\begin{figure}[ht]
\centering
\includegraphics[width=12cm,height=9cm]{./pics/sqtest4.eps}
\caption{Apprentissage fonction carree: 4 neurons, dataset 2}
\label{fig:sqtest4}
\end{figure}

\begin{figure}[ht]
\centering
\includegraphics[width=12cm,height=9cm]{./pics/chartsqtest4.eps}
\caption{Resultats apprentissage: 4 neurons, dataset 2}
\label{fig:chartsqtest4}
\end{figure}

Avec un dataset encore plus grande (100 inputs) et apres environ
30.000 iterations on arrive a obtenir une approximation suffisament
precise (fig \ref{chartsqtest5}) pour montrer notre point:
un perceptron multicouche avec une seule couche cache est suffisant pour
approximer la fonction carre (comme aussi n'importe quelle fonction continue) 
et la precisione depend strictement du nombre de neurons dans la couche cache.

\begin{figure}[ht]
\centering
\includegraphics[width=12cm,height=9cm]{./pics/chartsqtest5.eps}
\caption{Resultats apprentissage: 4 neurons, dataset 2}
\label{fig:chartsqtest5}
\end{figure}


%End content
\clearpage
\addcontentsline{toc}{section}{Références}
\bibliographystyle{plain}
\bibliography{rapport}

\end{document}
